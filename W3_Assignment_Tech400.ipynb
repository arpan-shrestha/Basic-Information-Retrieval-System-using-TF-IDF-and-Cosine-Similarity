{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jMpZErefvlkw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "leQRDEpQJKmf",
        "outputId": "db801f23-6014-4e26-de28-986b448eeb0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def term_frequency(term, document):\n",
        "    term_count = document.count(term)\n",
        "    total_terms = len(document)\n",
        "    return term_count / total_terms\n",
        "\n",
        "def inverse_document_frequency(term, all_documents):\n",
        "    num_docs_containing_term = sum(1 for document in all_documents if term in document)\n",
        "    return math.log(len(all_documents) / (1 + num_docs_containing_term))\n",
        "\n",
        "\n",
        "documents = []\n",
        "filenames= []\n",
        "corpus_dir = '/content/docs'\n",
        "\n",
        "for filename in os.listdir(corpus_dir):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        with open(os.path.join(corpus_dir, filename), 'r', encoding='utf-8') as file:\n",
        "            doc_text = file.read()\n",
        "            documents.append(preprocess(doc_text))\n",
        "            filenames.append(filename)\n",
        "\n",
        "queries = [\n",
        "    \"Tasmanian Aboriginal And PResident\",\n",
        "    \"Scientists typhoons and hurricanes Francis\",\n",
        "    \"Oceangateâ€™s Titan and Space\",\n",
        "]\n",
        "\n",
        "with open(\"result.txt\", \"w\") as result_file:\n",
        "    for query in queries:\n",
        "        processed_query = preprocess(query)\n",
        "\n",
        "        all_terms = set([term for doc in documents for term in doc]).union(set(processed_query))\n",
        "\n",
        "        tfidf_documents = []\n",
        "        for doc in documents:\n",
        "            tfidf_vector = []\n",
        "            for term in all_terms:\n",
        "                tf = term_frequency(term, doc)\n",
        "                idf = inverse_document_frequency(term, documents)\n",
        "                tfidf_vector.append(tf * idf)\n",
        "            tfidf_documents.append(tfidf_vector)\n",
        "\n",
        "        tfidf_query = []\n",
        "        for term in all_terms:\n",
        "            tf = term_frequency(term, processed_query)\n",
        "            idf = inverse_document_frequency(term, documents)\n",
        "            tfidf_query.append(tf * idf)\n",
        "\n",
        "        tfidf_documents = np.array(tfidf_documents)\n",
        "        tfidf_query = np.array([tfidf_query])\n",
        "\n",
        "        cosine_similarities = cosine_similarity(tfidf_query, tfidf_documents).flatten()\n",
        "\n",
        "        result_file.write(f\"Cosine similarities for Query: '{query}'\\n\")\n",
        "        for i, score in enumerate(cosine_similarities):\n",
        "            result_file.write(f\"Cosine similarity between Query and '{filenames[i]}': {score:.4f}\\n\")\n",
        "        result_file.write(\"\\n\")\n",
        "\n",
        "print(\"Cosine similarities written to result.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8tX9EZjAIAL",
        "outputId": "a8a4de35-00c1-4fbe-b54e-784292e0072a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarities written to result.txt\n"
          ]
        }
      ]
    }
  ]
}